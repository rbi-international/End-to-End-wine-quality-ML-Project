{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae4346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e817a94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ML_OPS_BABBY_FULL_STACK_NEW\\\\End-to-End-wine-quality-ML-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check present working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b2e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move back to Main folder\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab2e3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ML_OPS_BABBY_FULL_STACK_NEW\\\\End-to-End-wine-quality-ML-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check again\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6601c72",
   "metadata": {},
   "source": [
    "### Now change the variables in the `config/config.yaml` for `data_ingestion stage`\n",
    "\n",
    "- useful for you and next developer, change the values and the whole code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae73622",
   "metadata": {},
   "source": [
    "##### Now next step is to update schema.yaml update is there, but as of now it is not required, because we are not doing data validation , like what is the datatype of my data, how many columns are there etc, but we will write some dummy values like `key:value` so that error not comes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598a73e",
   "metadata": {},
   "source": [
    "##### Now next step is to update `params.yaml`, we will be keeping all the model parameters, and not hardcode them, similar to our `config.yaml.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828f3da",
   "metadata": {},
   "source": [
    "##### Now update the entity, as currently we are doing notebook experiment so we will not change in the config, then we try to copy paste in our modular coding.\n",
    "\n",
    "- entity is return type function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6607f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data ingestion with immutable attributes.\n",
    "    \n",
    "    This dataclass defines the required parameters for the data ingestion process\n",
    "    in the wine quality prediction pipeline. The 'frozen=True' parameter ensures\n",
    "    all attributes are read-only after initialization.\n",
    "    \n",
    "    Attributes:\n",
    "        root_dir (Path): Directory where all data ingestion outputs will be stored\n",
    "        source_URL (str): URL pointing to the source data to be downloaded\n",
    "        local_data_file (Path): Path where the downloaded data file will be saved\n",
    "        unzip_dir (Path): Directory where downloaded data will be extracted\n",
    "    \n",
    "    Note:\n",
    "        Using Path objects instead of strings ensures cross-platform compatibility\n",
    "        and provides helpful path manipulation methods.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a292c",
   "metadata": {},
   "source": [
    "##### Now update the `src/configuration.py` file, it will return all the configurations, which we have written in the previous step, and for that we have to use some libraries.\n",
    "\n",
    "##### But before it, we have to update the `constants` in which we have the constructor `__init__.py`, to return all the yaml file paths.\n",
    "\n",
    "##### Also, we have to use the common the `utils` in which we have the `common.py` file in which we have all the code which are common for all the modules, in this module, we will use `read_yaml` and `create_directories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6ec523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mlProject.constants import *\n",
    "from src.mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Manages configuration for the ML pipeline components.\n",
    "    \n",
    "    This class centralizes access to all configuration parameters by reading from\n",
    "    YAML configuration files and providing component-specific configuration objects.\n",
    "    \n",
    "    Attributes:\n",
    "        config: Main configuration parameters from config.yaml\n",
    "        params: Model hyperparameters and training parameters from params.yaml\n",
    "        schema: Data schema specifications from schema.yaml\n",
    "    \n",
    "    Methods:\n",
    "        get_data_ingestion_config: Returns configuration for the data ingestion component\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with paths to configuration files.\n",
    "        \n",
    "        Args:\n",
    "            config_filepath: Path to the main configuration file (default: CONFIG_FILE_PATH)\n",
    "            params_filepath: Path to the parameters file (default: PARAMS_FILE_PATH)\n",
    "            schema_filepath: Path to the schema file (default: SCHEMA_FILE_PATH)\n",
    "        \n",
    "        Note:\n",
    "            Creates the root artifacts directory specified in the main configuration.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Prepare and return the configuration for data ingestion.\n",
    "        \n",
    "        Returns:\n",
    "            DataIngestionConfig: Configuration object with all parameters\n",
    "                                 required for the data ingestion component.\n",
    "                                 \n",
    "        Note:\n",
    "            Creates the root directory for data ingestion if it doesn't exist.\n",
    "        \"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b6b74",
   "metadata": {},
   "source": [
    "##### Now we have to create the components for the data ingestion, and for that we have to import some of the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a75e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from src.mlProject import logger\n",
    "from src.mlProject.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ca72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    Handles the data ingestion process for the ML pipeline.\n",
    "    \n",
    "    This class is responsible for downloading the data from a source URL \n",
    "    and extracting it to a specified directory. It implements the first step\n",
    "    in the ML pipeline which is acquiring the raw data.\n",
    "    \n",
    "    Attributes:\n",
    "        config (DataIngestionConfig): Configuration containing all parameters\n",
    "                                      needed for the data ingestion process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DataIngestion component with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config (DataIngestionConfig): Configuration object with all required\n",
    "                                          parameters for data ingestion.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def download_file(self):\n",
    "        \"\"\"\n",
    "        Downloads the data file from the source URL.\n",
    "        \n",
    "        If the file already exists locally, the download is skipped.\n",
    "        Uses urllib.request to retrieve the file from the specified URL.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \n",
    "        Logs:\n",
    "            Info about successful download or existing file\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url = self.config.source_URL,\n",
    "                filename = self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\n",
    "\n",
    "\n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        Extracts the downloaded zip file into the data directory.\n",
    "        \n",
    "        Creates the extraction directory if it doesn't exist and\n",
    "        extracts all contents of the zip file to that location.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82584655",
   "metadata": {},
   "source": [
    "##### Now we have to create the pipeline. The execution flow, e.g., which method will execute first and after that the next method etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d21f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-08 08:13:41,421: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-08 08:13:41,424: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-08 08:13:41,432: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-08 08:13:41,435: INFO: common: created directory at: artifacts]\n",
      "[2025-05-08 08:13:41,437: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-05-08 08:13:43,665: INFO: 2625114949: artifacts/data_ingestion/data.zip download! with following info: \n",
      "Connection: close\n",
      "Content-Length: 23329\n",
      "Cache-Control: max-age=300\n",
      "Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox\n",
      "Content-Type: application/zip\n",
      "ETag: \"c69888a4ae59bc5a893392785a938ccd4937981c06ba8a9d6a21aa52b4ab5b6e\"\n",
      "Strict-Transport-Security: max-age=31536000\n",
      "X-Content-Type-Options: nosniff\n",
      "X-Frame-Options: deny\n",
      "X-XSS-Protection: 1; mode=block\n",
      "X-GitHub-Request-Id: 92AF:3D8379:E23A:23F1F:681C1A9E\n",
      "Accept-Ranges: bytes\n",
      "Date: Thu, 08 May 2025 02:44:46 GMT\n",
      "Via: 1.1 varnish\n",
      "X-Served-By: cache-maa10237-MAA\n",
      "X-Cache: MISS\n",
      "X-Cache-Hits: 0\n",
      "X-Timer: S1746672286.238488,VS0,VE297\n",
      "Vary: Authorization,Accept-Encoding,Origin\n",
      "Access-Control-Allow-Origin: *\n",
      "Cross-Origin-Resource-Policy: cross-origin\n",
      "X-Fastly-Request-ID: 40c8209a77b9b4460b326563d11f5cb46a3ece3a\n",
      "Expires: Thu, 08 May 2025 02:49:46 GMT\n",
      "Source-Age: 0\n",
      "\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Ingestion Pipeline\n",
    "\n",
    "This script orchestrates the data ingestion process by initializing the configuration,\n",
    "creating the data ingestion component, and executing the download and extraction steps.\n",
    "\n",
    "The pipeline follows these steps:\n",
    "1. Initialize the ConfigurationManager to load all configuration parameters\n",
    "2. Get the specific data ingestion configuration\n",
    "3. Initialize the DataIngestion component with the configuration\n",
    "4. Download the data file from the source URL\n",
    "5. Extract the zip file to the specified directory\n",
    "\n",
    "The entire process is wrapped in a try-except block to catch and propagate\n",
    "any exceptions that might occur during execution, ensuring proper error handling.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Initialize configuration\n",
    "    config = ConfigurationManager()\n",
    "    \n",
    "    # Get component-specific configuration\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    \n",
    "    # Initialize data ingestion component\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    \n",
    "    # Execute data download\n",
    "    data_ingestion.download_file()\n",
    "    \n",
    "    # Extract the downloaded zip file\n",
    "    data_ingestion.extract_zip_file()\n",
    "    \n",
    "except Exception as e:\n",
    "    # Propagate any exceptions for handling at a higher level\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbfe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
