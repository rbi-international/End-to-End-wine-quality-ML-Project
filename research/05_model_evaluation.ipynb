{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e140b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112bfbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ML_OPS_BABBY_FULL_STACK_NEW\\\\End-to-End-wine-quality-ML-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check present working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea27bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930fbd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ML_OPS_BABBY_FULL_STACK_NEW\\\\End-to-End-wine-quality-ML-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26239541",
   "metadata": {},
   "source": [
    "# creating entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680707d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for model evaluation with immutable attributes.\n",
    "    \n",
    "    This dataclass defines the required parameters for the model evaluation process\n",
    "    in the wine quality prediction pipeline. The 'frozen=True' parameter ensures\n",
    "    all attributes are read-only after initialization.\n",
    "    \n",
    "    Attributes:\n",
    "        root_dir (Path): Directory where all model evaluation outputs will be stored,\n",
    "                        including metrics and evaluation reports\n",
    "        test_data_path (Path): Path to the CSV file containing testing data,\n",
    "                              output from the data transformation step\n",
    "        model_path (Path): Path to the trained model file (.joblib),\n",
    "                          output from the model trainer step\n",
    "        all_params (dict): Dictionary containing all hyperparameters used for training,\n",
    "                          useful for documenting model configuration alongside metrics\n",
    "        metric_file_name (Path): Path where evaluation metrics will be saved as JSON,\n",
    "                                enables tracking performance across model iterations\n",
    "        target_column (str): Name of the column being predicted (the target variable),\n",
    "                            typically \"quality\" for the wine quality prediction\n",
    "    \n",
    "    Note:\n",
    "        This configuration combines paths from the config.yaml file, \n",
    "        hyperparameters from the params.yaml file, and target information \n",
    "        from the schema.yaml file into a single object for the model \n",
    "        evaluation component.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57528a0a",
   "metadata": {},
   "source": [
    "# configuration_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bafb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d83d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Manages configuration for the ML pipeline components.\n",
    "    \n",
    "    This class centralizes access to all configuration parameters by reading from\n",
    "    YAML configuration files and providing component-specific configuration objects.\n",
    "    \n",
    "    Attributes:\n",
    "        config: Main configuration parameters from config.yaml\n",
    "        params: Model hyperparameters and training parameters from params.yaml\n",
    "        schema: Data schema specifications from schema.yaml\n",
    "    \n",
    "    Methods:\n",
    "        get_data_ingestion_config: Returns configuration for the data ingestion component\n",
    "        get_data_validation_config: Returns configuration for the data validation component\n",
    "        get_data_transformation_config: Returns configuration for the data transformation component\n",
    "        get_model_trainer_config: Returns configuration for the model trainer component\n",
    "        get_model_evaluation_config: Returns configuration for the model evaluation component\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Initialize the ConfigurationManager with paths to configuration files.\n",
    "        \n",
    "        Args:\n",
    "            config_filepath: Path to the main configuration file (default: CONFIG_FILE_PATH)\n",
    "            params_filepath: Path to the parameters file (default: PARAMS_FILE_PATH)\n",
    "            schema_filepath: Path to the schema file (default: SCHEMA_FILE_PATH)\n",
    "        \n",
    "        Note:\n",
    "            Creates the root artifacts directory specified in the main configuration.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Prepare and return the configuration for model evaluation.\n",
    "        \n",
    "        This method combines information from three sources:\n",
    "        1. config.yaml - For file paths related to evaluation\n",
    "        2. params.yaml - For model hyperparameters to document with metrics\n",
    "        3. schema.yaml - For the target column name\n",
    "        \n",
    "        Returns:\n",
    "            ModelEvaluationConfig: Configuration object with all parameters\n",
    "                                  required for the model evaluation component.\n",
    "                                  \n",
    "        Note:\n",
    "            Creates the root directory for model evaluation if it doesn't exist.\n",
    "            Includes the complete hyperparameter dictionary in the configuration\n",
    "            to enable tracking parameters alongside performance metrics.\n",
    "        \"\"\"\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.ElasticNet\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            all_params=params,\n",
    "            metric_file_name=config.metric_file_name,\n",
    "            target_column=schema.name\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1c4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fb687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    \"\"\"\n",
    "    Handles the model evaluation process for the ML pipeline.\n",
    "    \n",
    "    This class is responsible for evaluating the trained model's performance\n",
    "    on test data, calculating key performance metrics, and saving these\n",
    "    metrics for future reference and model comparison.\n",
    "    \n",
    "    Attributes:\n",
    "        config (ModelEvaluationConfig): Configuration containing all parameters\n",
    "                                       needed for the model evaluation process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        \"\"\"\n",
    "        Initialize the ModelEvaluation component with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config (ModelEvaluationConfig): Configuration object with all required\n",
    "                                          parameters for model evaluation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def eval_metrics(self, actual, pred):\n",
    "        \"\"\"\n",
    "        Calculate key regression evaluation metrics.\n",
    "        \n",
    "        Computes three standard metrics for regression model performance:\n",
    "        - Root Mean Squared Error (RMSE): Measures the average magnitude of errors\n",
    "        - Mean Absolute Error (MAE): Measures the average absolute difference \n",
    "        - R-squared (R²): Measures the proportion of variance explained by the model\n",
    "        \n",
    "        Args:\n",
    "            actual: The true target values (ground truth)\n",
    "            pred: The predicted values from the model\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (rmse, mae, r2) - A tuple containing the three computed metrics\n",
    "            \n",
    "        Note:\n",
    "            Lower values are better for RMSE and MAE, while higher values (closer to 1)\n",
    "            are better for R².\n",
    "        \"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "    \n",
    "\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on test data and saves performance metrics.\n",
    "        \n",
    "        This method:\n",
    "        1. Loads the test data and trained model\n",
    "        2. Makes predictions on the test data\n",
    "        3. Calculates performance metrics (RMSE, MAE, R²)\n",
    "        4. Saves the metrics to a JSON file\n",
    "        \n",
    "        Process:\n",
    "        - Features: All columns except the target column\n",
    "        - Target: The specified target column from schema\n",
    "        - Metrics: RMSE, MAE, and R² are calculated and saved\n",
    "        - Storage: Metrics are saved in JSON format for later reference\n",
    "        \n",
    "        Returns:\n",
    "            None: The metrics are saved to disk but not returned\n",
    "            \n",
    "        Note:\n",
    "            This method could be extended to save visualizations of model\n",
    "            performance or additional metrics depending on project needs.\n",
    "        \"\"\"\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "\n",
    "        test_x = test_data.drop([self.config.target_column], axis=1)\n",
    "        test_y = test_data[[self.config.target_column]]\n",
    "        \n",
    "        predicted_qualities = model.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\n",
    "        \n",
    "        # Saving metrics as local\n",
    "        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "        save_json(path=Path(self.config.metric_file_name), data=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdf7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-08 13:06:14,659: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-08 13:06:14,664: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-08 13:06:14,670: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-08 13:06:14,672: INFO: common: created directory at: artifacts]\n",
      "[2025-05-08 13:06:14,672: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-05-08 13:06:14,804: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Pipeline\n",
    "\n",
    "This script orchestrates the model evaluation process by initializing the configuration,\n",
    "creating the model evaluation component, and executing the evaluation process.\n",
    "\n",
    "The pipeline follows these steps:\n",
    "1. Initialize the ConfigurationManager to load all configuration parameters\n",
    "2. Get the specific model evaluation configuration, which combines:\n",
    "   - Path information from config.yaml\n",
    "   - Hyperparameters from params.yaml\n",
    "   - Target column information from schema.yaml\n",
    "3. Initialize the ModelEvaluation component with the configuration\n",
    "4. Evaluate the model on test data and save the resulting metrics to a JSON file\n",
    "\n",
    "The entire process is wrapped in a try-except block to catch and propagate\n",
    "any exceptions that might occur during execution, ensuring proper error handling.\n",
    "\n",
    "Note:\n",
    "- This is the final stage in the ML pipeline, following model training\n",
    "- It calculates key performance metrics (RMSE, MAE, R²) to assess model quality\n",
    "- The metrics are saved to a JSON file for future reference and model comparison\n",
    "- There's a variable naming issue in this code: model_evaluation_config is used\n",
    "  both for the configuration object and the ModelEvaluation instance\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Initialize configuration\n",
    "    config = ConfigurationManager()\n",
    "    \n",
    "    # Get component-specific configuration\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    \n",
    "    # Initialize model evaluation component\n",
    "    # Note: Variable naming issue - reusing model_evaluation_config for the evaluator instance\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    \n",
    "    # Execute model evaluation and save metrics\n",
    "    model_evaluation_config.save_results()\n",
    "    \n",
    "except Exception as e:\n",
    "    # Propagate any exceptions for handling at a higher level\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318e396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
